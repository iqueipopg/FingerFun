{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 4: Detección de Movimiento y Seguimiento de Objetos\n",
    "\n",
    "**Asignatura:** Visión por Ordenador I  \n",
    "**Grado:** Ingeniería Matemática e Inteligencia Artificial  \n",
    "**Curso:** 2024/2025  \n",
    "\n",
    "### Objetivo\n",
    "En esta práctica, aprenderás a implementar algoritmos de detección de movimiento mediante sustracción de fondo y flujo óptico, y explorarás el filtro de Kalman para el seguimiento de objetos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Materiales\n",
    "- **Python 3.8+**\n",
    "- **OpenCV**: Puedes instalarlo con `pip install opencv-python`\n",
    "- **Dataset de video**: Se usará un archivo de video o la cámara en tiempo real para probar los métodos de detección de movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado A: Sustracción de Fondo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea A.1: Carga de Video\n",
    "Carga un video en el cual se detectarán objetos en movimiento. Puedes utilizar\n",
    "un video local o la cámara en tiempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a method that reads a video file (using VideoCapture from OpenCV) and returns its frames along with video properties\n",
    "def read_video(videopath):\n",
    "    \"\"\"\n",
    "    Reads a video file and returns its frames along with video properties.\n",
    "\n",
    "    Args:\n",
    "        videopath (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - frames (list): A list of frames read from the video.\n",
    "            - frame_width (int): The width of the video frames.\n",
    "            - frame_height (int): The height of the video frames.\n",
    "            - frame_rate (float): The frame rate of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: Complete this line to read the video file\n",
    "    cap = cv2.VideoCapture(videopath) \n",
    "    #TODO: Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print('Error: Could not open the video file')\n",
    "\n",
    "    #TODO: Get the szie of frames and the frame rate of the video\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # Get the width of the video frames\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Get the height of the video frames\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS) # Get the frame rate of the video\n",
    "    \n",
    "    #TODO: Use a loop to read the frames of the video and store them in a list\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames, frame_width, frame_height, frame_rate\n",
    "\n",
    "#TODO: Path to the video file (visiontraffic.avi)\n",
    "videopath = os.path.join('../data',  'visiontraffic.avi')\n",
    "\n",
    "\n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea A.2: Sustración de Fondo mediante diferencia de frames\n",
    "Realiza una sustracción de fondo mediante diferencia de frames, para ello guarda\n",
    "un frame con el fondo estático y úsalo como frame de referencia de fondo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 0 selected as reference frame\n"
     ]
    }
   ],
   "source": [
    "#TODO:  Show the frames to select the reference frame, press 'n' to move to the next frame and 's' to select the frame\n",
    "for i, frame in enumerate(frames):\n",
    "    #TODO: Show the frame\n",
    "    cv2.imshow('Video', frame)\n",
    "    # Wait for the key\n",
    "    key = cv2.waitKey(0)\n",
    "    # If the key is 'n' continue to the next frame\n",
    "    if key == ord('n'):\n",
    "        continue\n",
    "    # If the key is 's' select the frame as the reference frame\n",
    "    elif key == ord('s'):\n",
    "        #TODO: Copy the frame to use it as a reference\n",
    "        reference_frame = frame.copy()\n",
    "        #TODO: Convert the reference frame to grayscale\n",
    "        reference_frame = cv2.cvtColor(reference_frame, cv2.COLOR_BGR2GRAY)\n",
    "        print('Frame {} selected as reference frame'.format(i))\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#TODO: Compute the difference between the reference frame and the rest of the frames and show the difference\n",
    "for frame in frames:\n",
    "    # Convert the frame to grayscale\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #TODO: Compute the difference between the reference frame and the current frame\n",
    "    diff = cv2.absdiff(reference_frame, frame)\n",
    "    cv2.imshow('Diferencia', diff)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea A.3: Configuración de la sustración de fondo con GMM\n",
    "Configura el sustractor de fondo usando el modelo de mezcla de gaussianas\n",
    "adaptativas (MOG2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use MOG2 to detect the moving objects in the video\n",
    "\n",
    "history = 500 # Number of frames to use to build the background model\n",
    "varThreshold = 16  # Threshold to detect the background\n",
    "detectShadows = True  # If True the algorithm detects the shadows\n",
    "\n",
    "#TODO: Create the MOG2 object\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2(history, varThreshold, detectShadows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea A.3: Aplicación de la Sustracción de Fondo\n",
    "\n",
    "Aplica la sustracción de fondo en cada frame para extraer los objetos en movimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use a loop to detect the moving objects in the video using the MOG2 algorithm and \n",
    "# save a video storing the parameters at the name of the file\n",
    "\n",
    "#TODO: Create a folder to store the videos\n",
    "output_folder = 'output_videos'\n",
    "folder_path = os.path.join('../data', output_folder)\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "#TODO: Name of the output video file with the parameters (history, varThreshold, detectShadows)\n",
    "videoname = f'../data/output_videos/output_{history}_{varThreshold}_{detectShadows}.avi' # Name of the output video file with the parameters\n",
    "\n",
    "\n",
    "#TODO: Create a VideoWriter object to save the video\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MJPG') # Codec to use\n",
    "frame_size = (frame_width, frame_height) # Size of the frame\n",
    "fps = frame_rate # Frame rate of the video\n",
    "out = cv2.VideoWriter(videoname, fourcc, frame_rate, frame_size)\n",
    "\n",
    "# masks = []\n",
    "for frame in frames:\n",
    "    #TODO: Apply the MOG2 algorithm to detect the moving objects\n",
    "    mask = mog2.apply(frame)\n",
    "    #TODO: Convert to BGR the mask to store it in the video\n",
    "    mask = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "    #TODO: Save the mask in a video\n",
    "    # masks.append(mask)\n",
    "    out.write(mask)\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado A**\n",
    "1. ¿Cómo afecta la variable `varThreshold` a la precisión de la detección?\n",
    "2. ¿Qué ventajas presenta `createBackgroundSubtractorMOG2` frente a métodos simples de diferencia de imágenes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.1\n",
    "varThreshold controla la sensibilidad para detectar movimiento; valores bajos aumentan detección pero generan ruido, valores altos reducen sensibilidad.\n",
    "\n",
    "A.2\n",
    "createBackgroundSubtractorMOG2 se adapta a cambios de iluminación y fondos complejos, a diferencia de la simple diferencia de imágenes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado B: Flujo Óptico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea B.1: Configuración del Flujo Óptico\n",
    "\n",
    "Consulta la documentación de cv2.calcOpticalFlowPyrLK para ver que parametros se deben definir para realizar el calculo del flujo óptico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the method to read the video file (slow_traffic_small.avi)\n",
    "videopath = os.path.join('../data',  'slow_traffic_small.mp4')\n",
    "\n",
    "\n",
    "videopath = videopath  # Path to the video file\n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)\n",
    "\n",
    "#TODO: Define the parameters for Lucas-Kanade optical flow\n",
    "winSize=(15, 15)\n",
    "maxLevel= 2\n",
    "criteria= (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea B.2: Detección de Puntos de Interés\n",
    "\n",
    "Detecta los puntos de interés iniciales usando el algoritmo de Shi-Tomasi (cv2.goodFeaturesToTrack) en el primer frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Detect the initial points of interest in the first frame\n",
    "\n",
    "#TODO: Convert the first frame to grayscale\n",
    "prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)\n",
    "# prev_gray = prev_gray.astype(np.float32)\n",
    "\n",
    "\n",
    "#TODO: Define the parameters of the Shi-Tomasi algorithm\n",
    "mask = None\n",
    "maxCorners = 100\n",
    "qualityLevel = 0.3\n",
    "minDistance = 7\n",
    "blockSize = 7\n",
    "\n",
    "# Use the function goodFeaturesToTrack to detect the points of interest\n",
    "p0 = cv2.goodFeaturesToTrack(prev_gray, mask=mask, maxCorners=maxCorners, qualityLevel=qualityLevel, minDistance=minDistance, blockSize=blockSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea B.3: Cálculo y Visualización del Flujo Óptico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use a loop to track the points of interest in the rest of the frames\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(frame)\n",
    "\n",
    "for i, frame in enumerate(frames[1:]):\n",
    "    #TODO: Copy the frame\n",
    "    input_frame = frame.copy()\n",
    "    # Convert the frame to grayscale\n",
    "    frame_gray = cv2.cvtColor(input_frame, cv2.COLOR_BGR2GRAY)\n",
    "    #TODO: Calculate the optical flow using the Lucas-Kanade algorithm\n",
    "\n",
    "    p1, st, err = cv2.calcOpticalFlowPyrLK(prev_gray, frame_gray, p0, None, winSize=winSize, maxLevel=maxLevel, criteria=criteria)\n",
    "\n",
    "    # Select the points that were successfully tracked\n",
    "    good_new = p1[st == 1]\n",
    "    good_old = p0[st == 1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel().astype(int)\n",
    "        c, d = old.ravel().astype(int)\n",
    "        input_frame = cv2.circle(input_frame, (a, b), 5, (0, 0, 255), -1)\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)\n",
    "\n",
    "    #TODO: Update the inputs for the next iteration\n",
    "    prev_gray = frame_gray # Copy the current frame to the previous frame\n",
    "    p0 = p1 # Update the points to track\n",
    "    # Show the frame with the tracks\n",
    "    cv2.imshow('Frame', cv2.add(input_frame, mask))\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado B**\n",
    "1. ¿Qué efecto tiene el parámetro `winSize` en la precisión del flujo óptico?\n",
    "2. ¿Cómo influye el parámetro `qualityLevel` en la función `cv2.goodFeaturesToTrack` al detectar puntos de interés?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B.1\n",
    "winSize ajusta la precisión del flujo óptico: tamaños grandes mejoran precisión, pero aumentan el costo computacional.\n",
    "\n",
    "B.2\n",
    "qualityLevel define el mínimo de calidad para puntos de interés; valores altos mejoran precisión pero reducen cantidad de puntos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado C: Filtro de Kalman para Seguimiento de Objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea C.1: Configuración del Filtro de Kalman\n",
    "\n",
    "Inicializa el filtro de Kalman (cv2.KalmanFilter) con una matriz de medición y transición adecuada para un seguimiento en dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create a method that reads a video file (using VideoCapture from OpenCV) and returns its frames along with video properties\n",
    "def read_video(videopath):\n",
    "    \"\"\"\n",
    "    Reads a video file and returns its frames along with video properties.\n",
    "\n",
    "    Args:\n",
    "        videopath (str): The path to the video file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - frames (list): A list of frames read from the video.\n",
    "            - frame_width (int): The width of the video frames.\n",
    "            - frame_height (int): The height of the video frames.\n",
    "            - frame_rate (float): The frame rate of the video.\n",
    "    \"\"\"\n",
    "\n",
    "    #TODO: Complete this line to read the video file\n",
    "    cap = cv2.VideoCapture(videopath) \n",
    "    #TODO: Check if the video was successfully opened\n",
    "    if not cap.isOpened():\n",
    "        print('Error: Could not open the video file')\n",
    "\n",
    "    #TODO: Get the szie of frames and the frame rate of the video\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) # Get the width of the video frames\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # Get the height of the video frames\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS) # Get the frame rate of the video\n",
    "    \n",
    "    #TODO: Use a loop to read the frames of the video and store them in a list\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return frames, frame_width, frame_height, frame_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial position selected: 330, 211\n"
     ]
    }
   ],
   "source": [
    "#TODO: Use the method to read the video file (slow_traffic_small.avi)\n",
    "videopath = os.path.join('../data',  'slow_traffic_small.mp4')\n",
    "frames, frame_width, frame_height, frame_rate = read_video(videopath)\n",
    "\n",
    "# TODO: Create the Kalman filter object\n",
    "kf = cv2.KalmanFilter(4, 2)  # 4 estados (x, y, vx, vy) y 2 mediciones (x, y)\n",
    "\n",
    "# TODO: Initialize the state of the Kalman filter\n",
    "kf.measurementMatrix = np.array([[1, 0, 0, 0], [0, 1, 0, 0]], np.float32)  # Matriz de medición 2x4\n",
    "kf.transitionMatrix = np.array([[1, 0, 1, 0], [0, 1, 0, 1], [0, 0, 1, 0], [0, 0, 0, 1]], np.float32)  # Matriz de transición 4x4\n",
    "kf.processNoiseCov = np.eye(4, dtype=np.float32) * 1e-2  # Covarianza de ruido de proceso 4x4\n",
    "\n",
    "measurement = np.array((2, 1), np.float32)\n",
    "prediction = np.zeros((2, 1), np.float32)\n",
    "\n",
    "# TODO: Show the frames to select the initial position of the object\n",
    "for i, frame in enumerate(frames):\n",
    "    # Show the frame\n",
    "    cv2.imshow('Frame', frame)\n",
    "    # Wait for the key\n",
    "    key = cv2.waitKey(0)\n",
    "    # If the key is 'n' continue to the next frame\n",
    "    if key == ord('n'):\n",
    "        continue\n",
    "    # If the key is 's' select the position of the object\n",
    "    elif key == ord('s'):\n",
    "        # Select the position of the object\n",
    "        x, y, w, h = cv2.selectROI('Frame', frame, False)\n",
    "        track_window = (x, y, w, h)\n",
    "        \n",
    "        # TODO: Compute the center of the object\n",
    "        cx = x + w // 2\n",
    "        cy = y + h // 2\n",
    "        \n",
    "        # TODO: Initialize the state of the Kalman filter\n",
    "        kf.statePost = np.array([[cx], [cy], [0], [0]], np.float32)\n",
    "\n",
    "        # Initialize the covariance matrix\n",
    "        kf.errorCovPost = np.eye(4, dtype=np.float32)\n",
    "        \n",
    "        # Predict the position of the object\n",
    "        prediction = kf.predict()\n",
    "        \n",
    "        # TODO: Update the measurement and correct the Kalman filter\n",
    "        measurement = np.array([[cx], [cy]], np.float32)\n",
    "        kf.correct(measurement)\n",
    "\n",
    "        # TODO: Crop the object\n",
    "        crop = frame[y:y+h, x:x+w].copy()\n",
    "        \n",
    "        # TODO: Convert the cropped object to HSV\n",
    "        hsv_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # TODO: Compute the histogram of the cropped object (Reminder: Use only the Hue channel (0-180))\n",
    "        crop_hist = cv2.calcHist([hsv_crop], [0], mask=None, histSize=[180], ranges=[0, 180])\n",
    "        cv2.normalize(crop_hist, crop_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "        \n",
    "        print(f'Initial position selected: {x}, {y}')\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tarea C.2: Predicción y Corrección del Estado\n",
    "\n",
    "Realiza la predicción del estado y corrige la posición estimada en cada iteración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use the Kalman filter to predict the position of the points of interest\n",
    "\n",
    "term_crit = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 1)\n",
    "\n",
    "for frame in frames[i:]:\n",
    "    #TODO: Copy the frame \n",
    "    input_frame = frame.copy()\n",
    "    #TODO: Convert the frame to HSV\n",
    "    img_hsv = cv2.cvtColor(input_frame, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Compute the back projection of the histogram\n",
    "    img_bproject = cv2.calcBackProject([img_hsv], [0], crop_hist, [0, 180], 1)\n",
    "    \n",
    "    # Apply the mean shift algorithm to the back projection\n",
    "    ret, track_window = cv2.meanShift(img_bproject, track_window, term_crit)\n",
    "    x_,y_,w_,h_ = track_window\n",
    "    #TODO: Compute the center of the object\n",
    "    c_x = x_ + w_ // 2\n",
    "    c_y = y_ + h_ // 2\n",
    "    \n",
    "    # Predict the position of the object\n",
    "    prediction = kf.predict()\n",
    "\n",
    "    #TODO: Update the measurement and correct the Kalman filter\n",
    "    measurement = np.array([[c_x], [c_y]], np.float32)\n",
    "    kf.correct(measurement)\n",
    "\n",
    "    \n",
    "    # Draw the predicted position\n",
    "    cv2.circle(input_frame, (int(prediction[0][0]), int(prediction[1][0])), 5, (0, 0, 255), -1)\n",
    "    cv2.circle(input_frame, (int(c_x), int(c_y)), 5, (0, 255, 0), -1)\n",
    "    cv2.rectangle(input_frame, (x_, y_), (x_ + w_, y_ + h_), (255, 0, 0), 2)\n",
    "    \n",
    "    # Show the frame with the predicted position\n",
    "    cv2.imshow('Frame', input_frame)\n",
    "    key = cv2.waitKey(0)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Apartado C**\n",
    "1. ¿Cómo afecta el valor de `transitionMatrix` a la predicción en el filtro de Kalman?\n",
    "2. ¿Cuál es la diferencia entre `measurementMatrix` y `transitionMatrix` en el contexto del seguimiento de objetos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C.1\n",
    "transitionMatrix afecta la predicción de movimiento del objeto, influyendo en la precisión del seguimiento en Kalman.\n",
    "Con una transitionMatrix de 0.5 la predicción de movimiento resulta muy equivocada. Si ponemos un valor de 0.8, la predicción sigue un movimiento correcto y se asemeja a la real pero con cierto desfase inicial.\n",
    "\n",
    "C.2\n",
    "measurementMatrix ajusta el estado con mediciones, mientras transitionMatrix predice el movimiento del objeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio Adicional: Exploración del Modelo de Mezcla de Gaussianas (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objetivo**: Investiga cómo funciona el modelo de mezcla de gaussianas (GMM) para mejorar la detección en condiciones de iluminación cambiantes.\n",
    "\n",
    "1. Implementación del GMM: Utiliza `cv2.createBackgroundSubtractorMOG()` y ajusta el parámetro `history` para observar cómo cambia la detección en función de la duración de la memoria del fondo.\n",
    "2. Comparación con `MOG2`: Observa las diferencias en la sensibilidad a las sombras y los cambios de iluminación. Prueba con videos que incluyan cambios graduales de iluminación y objetos que se detienen temporalmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath = os.path.join('../data',  'slow_traffic_small.mp4')\n",
    "cap = cv2.VideoCapture(videopath)\n",
    "\n",
    "history = 10\n",
    "history2 = 300\n",
    "\n",
    "background_subtractor = cv2.bgsegm.createBackgroundSubtractorMOG(history=history, nmixtures=5, backgroundRatio=0.7, noiseSigma=0)\n",
    "background_subtractor2 = cv2.bgsegm.createBackgroundSubtractorMOG(history=history2, nmixtures=5, backgroundRatio=0.7, noiseSigma=0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Aplicar el sustractor de fondo al fotograma\n",
    "    fg_mask = background_subtractor.apply(frame)\n",
    "    fg_mask2 = background_subtractor2.apply(frame)\n",
    "\n",
    "    # Mostrar el resultado\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fg_mask)\n",
    "    cv2.imshow('Foreground Mask 2', fg_mask2)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath = os.path.join('../data',  'slow_traffic_small.mp4')\n",
    "cap = cv2.VideoCapture(videopath)\n",
    "\n",
    "history = 200\n",
    "\n",
    "mog = cv2.bgsegm.createBackgroundSubtractorMOG(history=history)\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2(history=history, detectShadows=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    fg_mask_mog = mog.apply(frame)\n",
    "    fg_mask_mog2 = mog2.apply(frame)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Foreground Mask MOG', fg_mask_mog)\n",
    "    cv2.imshow('Foreground Mask MOG2', fg_mask_mog2)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas del Ejercicio Adicional**\n",
    "1. ¿Qué ventajas observas en `createBackgroundSubtractorMOG` en comparación con `createBackgroundSubtractorMOG2`?\n",
    "2. ¿Cómo afecta el parámetro `history` al rendimiento de detección en escenas con objetos que aparecen y desaparecen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "videopath = os.path.join('../data/test_videos',  'vtest.avi')\n",
    "cap = cv2.VideoCapture(videopath)\n",
    "\n",
    "history = 200\n",
    "\n",
    "mog = cv2.bgsegm.createBackgroundSubtractorMOG(history=history)\n",
    "mog2 = cv2.createBackgroundSubtractorMOG2(history=history, detectShadows=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    fg_mask_mog = mog.apply(frame)\n",
    "    fg_mask_mog2 = mog2.apply(frame)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Foreground Mask MOG', fg_mask_mog)\n",
    "    cv2.imshow('Foreground Mask MOG2', fg_mask_mog2)\n",
    "\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.1\n",
    "createBackgroundSubtractorMOG es más sencillo y consume menos recursos, pero MOG2 maneja sombras y cambios de iluminación con mayor precisión.\n",
    "EL BackgroundSubtractorMOG2 hace un mucho mejor trabajo detectando las sombras y las diferencias de iluminación del vídeo. Es más preciso y detecta como frente los elementos de manera más concisa. También hay que tener en cuenta que ante los cambios de iluminación puede provocar un 'fogonazo' de píxeles que asume como frente cuando no lo son. Al paso del tiempo, acaba absorbiéndolas y añadiéndoselas al fondo aunque de manera lenta. El  BackgroundSubtractorMOG no tiene esta clase de problemas tal y como se puede observar en el video de comparación que se ejecuta en la celda anterior.  \n",
    "\n",
    "D.2\n",
    "El parámetro history afecta la duración de la memoria del fondo; valores altos mejoran detección de objetos estables, pero ralentizan la respuesta a cambios rápidos.\n",
    "Se ha implementado un apartado dónde se generan 2 background substractors. Uno con 10 y otro con 300 de history. Aquél con 10 de histórico se actualiza mucho mas rápido que aquél de 300 y acaba absorbiendo cosas como si fuesen fondo (pintándolas en negro) mucho antes que aquél con un histórico mayor."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
